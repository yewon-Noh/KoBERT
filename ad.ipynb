{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRahxNeILzj7"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install boto3 --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "42cC58EjLzfN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\yewo2\\.conda\\envs\\kobert_\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\yewo2\\.conda\\envs\\kobert_\\lib\\site-packages\\requests\\__init__.py:78: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({0}) or chardet ({1}) doesn't match a supported \"\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DhgMUis2L15U"
      },
      "outputs": [],
      "source": [
        "from kobert import get_tokenizer\n",
        "from kobert import get_pytorch_kobert_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oZc4iR3xMWue"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=2,\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        else:\n",
        "            out = pooler\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "XEHMm47KLj2v"
      },
      "outputs": [],
      "source": [
        "# https://jimmy-ai.tistory.com/164\n",
        "\n",
        "## CPU\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "ko_model = torch.load('model.pt', map_location=device) # input으로 저장된 디렉토리만 지정하면 완료"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FAQY8pExNFFW"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8pL03a5M23K",
        "outputId": "2b163ed1-8816-4cb2-ddb2-2e5701e68759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using cached model. d:\\GitHub\\KoBERT\\.cache\\kobert_v1.zip\n",
            "using cached model. d:\\GitHub\\KoBERT\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ],
      "source": [
        "bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUYafOy_Mzlp",
        "outputId": "cd470a73-4eb9-4154-dcdb-007d760a40b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using cached model. d:\\GitHub\\KoBERT\\.cache\\kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ],
      "source": [
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Sto39m8yM7wv"
      },
      "outputs": [],
      "source": [
        "max_len = 100\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "X8fyQiJIK-Zh"
      },
      "outputs": [],
      "source": [
        "# https://tech-diary.tistory.com/31\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 위에서 설정한 tok, max_len, batch_size, device를 그대로 입력\n",
        "# comment : 예측하고자 하는 텍스트 데이터 리스트\n",
        "def getSentimentValue(comment):\n",
        "  commnetslist = [] # 텍스트 데이터를 담을 리스트\n",
        "  res_list = [] # 결과 값을 담을 리스트\n",
        "  for c in comment: # 모든 댓글\n",
        "    commnetslist.append( [c, 5] ) # [댓글, 임의의 양의 정수값] 설정\n",
        "    \n",
        "  pdData = pd.DataFrame( commnetslist, columns = [['text', 'label']] )\n",
        "  pdData = pdData.values\n",
        "  test_set = BERTDataset(pdData, 0, 1, tok, max_len, True, False) \n",
        "  test_input = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=0)\n",
        "  \n",
        "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_input):\n",
        "    token_ids = token_ids.long().to(device)\n",
        "    segment_ids = segment_ids.long().to(device)\n",
        "    valid_length= valid_length \n",
        "    # 이때, out이 예측 결과 리스트\n",
        "    out = ko_model(token_ids, valid_length, segment_ids)\n",
        "\t\n",
        "    # e는 2가지 실수 값으로 구성된 리스트\n",
        "    # 0번 인덱스가 더 크면 일반, 광고는 반대\n",
        "    for e in out:\n",
        "      if e[0]>e[1]: # 부정\n",
        "        value = 0\n",
        "      else: #긍정(광고)\n",
        "        value = 1\n",
        "      res_list.append(value)\n",
        "\n",
        "  return res_list # 텍스트 데이터에 1대1 매칭되는 감성값 리스트 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XJ7oKkjL8KQ",
        "outputId": "6ac28522-35a7-490e-d0a8-1012f5a13177"
      },
      "outputs": [],
      "source": [
        "test_ = ['여러 클렌징도구기계 써봤는데 요거 느무 좋다   아 잇치치약도 굿       프라엘초음파클렌저 프라엘 클렌징기계 써모슈티컬 잇치', \n",
        "         '이히히히힣힠        대놓고 자랑중입니다    프라엘 남편최고', \n",
        "         'lgpral 4th EVENT 코어탄력을 세우는 LG프라엘 공식판매처 알로이비즈 입니다 5월 스페셜 이벤트 이 게시글에 댓글과 함께 친구를 태그 해주세요 추첨을 통해 프라엘 듀얼모션클렌저 레꼴뜨 스마일베이커미니 메디힐 마스크팩을 선물로 드립니다 참여방법 lgpral 팔로우 꾸욱 이벤트 게시글 좋아요 누르고 친구 태그와 함께 댓글 남기기 리그램 하신 후에 엘지프라엘 알로이비즈 프라엘 lg프라엘 이벤트 인스타이벤트 선물이벤트 선물증정 등 해시태그 달기 Tip 지인 태그 여러명 하시면 당첨확률 UP 증정선물 프라엘 듀얼모션클렌저 1명 색상랜덤 레꼴뜨 스마일베이커미니 2명 색상랜덤 메디힐 마스크팩 10매 8명 종류랜덤 이벤트 기간 517527 당첨자 발표 67 금 당첨자 수 11명 선물 증정은 당첨자 발표 후 다음날 발송해 드립니다 궁금하신 점은 언제든지 DM으로 문의 주세요 2019년엔프라엘로더욱아름다워지세요 LG프라엘 풀세트하나면 내일더예쁨예약 프라엘 엘지프라엘 lg프라엘 led마스크 프라엘마스크 피부관리 llg프라엘마스크 엘지프라엘더마led마스크 이나영마스크 이나영 뷰티디바이스 이벤트 리그램이벤트 리그램 선물이벤트 경품이벤트']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_ = ['노트북 뭐 살까 고민하는 친구들~~! 은하수 이온 추천해! 발열도 적고 15인치에 선명하다. #협찬아님']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_ = getSentimentValue(test_)\n",
        "result_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G9ptxX7cjib"
      },
      "source": [
        "## model summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31mkGdcGiHrE"
      },
      "outputs": [],
      "source": [
        "# https://velog.io/@springkim/pytorch-model-summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3UVl1y_bUWe",
        "outputId": "c032193a-14c8-4ddd-ebf3-d24cdb53cd3d"
      },
      "outputs": [],
      "source": [
        "# !pip install pytorch-model-summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import pytorch_model_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMg6KuqTbzJU",
        "outputId": "490daf99-c6fc-46a6-bb42-bfc3bb68c4c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------\n",
            "           Layer (type)          Input Shape         Param #     Tr. Param #\n",
            "=============================================================================\n",
            "               Conv2d-1     [1, 3, 256, 256]          23,296          23,296\n",
            "                 ReLU-2      [1, 64, 63, 63]               0               0\n",
            "            MaxPool2d-3      [1, 64, 63, 63]               0               0\n",
            "               Conv2d-4      [1, 64, 31, 31]         307,392         307,392\n",
            "                 ReLU-5     [1, 192, 31, 31]               0               0\n",
            "            MaxPool2d-6     [1, 192, 31, 31]               0               0\n",
            "               Conv2d-7     [1, 192, 15, 15]         663,936         663,936\n",
            "                 ReLU-8     [1, 384, 15, 15]               0               0\n",
            "               Conv2d-9     [1, 384, 15, 15]         884,992         884,992\n",
            "                ReLU-10     [1, 256, 15, 15]               0               0\n",
            "              Conv2d-11     [1, 256, 15, 15]         590,080         590,080\n",
            "                ReLU-12     [1, 256, 15, 15]               0               0\n",
            "           MaxPool2d-13     [1, 256, 15, 15]               0               0\n",
            "   AdaptiveAvgPool2d-14       [1, 256, 7, 7]               0               0\n",
            "             Dropout-15            [1, 9216]               0               0\n",
            "              Linear-16            [1, 9216]      37,752,832      37,752,832\n",
            "                ReLU-17            [1, 4096]               0               0\n",
            "             Dropout-18            [1, 4096]               0               0\n",
            "              Linear-19            [1, 4096]      16,781,312      16,781,312\n",
            "                ReLU-20            [1, 4096]               0               0\n",
            "              Linear-21            [1, 4096]       4,097,000       4,097,000\n",
            "=============================================================================\n",
            "Total params: 61,100,840\n",
            "Trainable params: 61,100,840\n",
            "Non-trainable params: 0\n",
            "-----------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------\n",
            "           Layer (type)         Output Shape         Param #     Tr. Param #\n",
            "=============================================================================\n",
            "               Conv2d-1      [1, 64, 63, 63]          23,296          23,296\n",
            "                 ReLU-2      [1, 64, 63, 63]               0               0\n",
            "            MaxPool2d-3      [1, 64, 31, 31]               0               0\n",
            "               Conv2d-4     [1, 192, 31, 31]         307,392         307,392\n",
            "                 ReLU-5     [1, 192, 31, 31]               0               0\n",
            "            MaxPool2d-6     [1, 192, 15, 15]               0               0\n",
            "               Conv2d-7     [1, 384, 15, 15]         663,936         663,936\n",
            "                 ReLU-8     [1, 384, 15, 15]               0               0\n",
            "               Conv2d-9     [1, 256, 15, 15]         884,992         884,992\n",
            "                ReLU-10     [1, 256, 15, 15]               0               0\n",
            "              Conv2d-11     [1, 256, 15, 15]         590,080         590,080\n",
            "                ReLU-12     [1, 256, 15, 15]               0               0\n",
            "           MaxPool2d-13       [1, 256, 7, 7]               0               0\n",
            "   AdaptiveAvgPool2d-14       [1, 256, 6, 6]               0               0\n",
            "             Dropout-15            [1, 9216]               0               0\n",
            "              Linear-16            [1, 4096]      37,752,832      37,752,832\n",
            "                ReLU-17            [1, 4096]               0               0\n",
            "             Dropout-18            [1, 4096]               0               0\n",
            "              Linear-19            [1, 4096]      16,781,312      16,781,312\n",
            "                ReLU-20            [1, 4096]               0               0\n",
            "              Linear-21            [1, 1000]       4,097,000       4,097,000\n",
            "=============================================================================\n",
            "Total params: 61,100,840\n",
            "Trainable params: 61,100,840\n",
            "Non-trainable params: 0\n",
            "-----------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "net = torchvision.models.alexnet()\n",
        "\n",
        "# show_input : True -> 입력크기, False -> 출력크기\n",
        "print(pytorch_model_summary.summary(net, torch.zeros(1, 3, 256, 256), show_input=True))\n",
        "print(pytorch_model_summary.summary(net, torch.zeros(1, 3, 256, 256), show_input=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8msBBHP2i9Vl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "pytorch_kobert_use.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "c584cb0d2ff0d23d7f365633096f2d7d5596f7296676c1917c80604c9d66a1fe"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('kobert_')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
